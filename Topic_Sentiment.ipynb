{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Modules and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU numbers: 32\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt  \n",
    "# import nltk\n",
    "# from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "from tqdm import tqdm\n",
    "# from nltk.corpus import stopwords\n",
    "# stopwordEn = stopwords.words('english')\n",
    "# from nltk.corpus import wordnet\n",
    "import pickle\n",
    "import multiprocessing as mp\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.test.utils import datapath\n",
    "import pyLDAvis.gensim\n",
    "import time\n",
    "\n",
    "print('CPU numbers:',mp.cpu_count())\n",
    "def _apply_df(args):\n",
    "    df, func, kwargs = args\n",
    "    return df.apply(func, **kwargs)\n",
    "def apply_by_multiprocessing(df, func, **kwargs):\n",
    "#     print(kwargs)\n",
    "    workers = kwargs.pop('workers')\n",
    "    pool = mp.Pool(processes=workers)\n",
    "    result = pool.map(_apply_df, [(d, func, kwargs) for d in np.array_split(df, workers)])\n",
    "    pool.close()\n",
    "    return pd.concat(list(result))\n",
    "#apply_by_multiprocessing(fullset['Text'], processText, workers=cores)\n",
    "def lemmaWord(word):\n",
    "    lemma = wordnet.morphy(word)\n",
    "    if lemma is not None:\n",
    "        return lemma\n",
    "    else:\n",
    "        return word\n",
    "def processText(text,lemma=False, gram=1):\n",
    "    text = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b|@\\w+|#', '', text, flags=re.MULTILINE) #delete URL, #hashtag# , and @xxx\n",
    "    tokens = word_tokenize(text)\n",
    "    whitelist = [\"n't\", \"not\", \"no\"]\n",
    "    tokens = [lemmaWord(i.lower()) if lemma else i.lower() for i in tokens if (i.lower() not in stopwordEn or i.lower() in whitelist) and i.isalpha()]\n",
    "    if gram<=1:\n",
    "        return tokens\n",
    "    else:\n",
    "        return [' '.join(i) for i in nltk.ngrams(tokens, gram)]\n",
    "    \n",
    "dates = range(23,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:25<00:00,  3.67s/it]\n"
     ]
    }
   ],
   "source": [
    "# dfs_news, dfs_tweets={},{}\n",
    "\n",
    "# for d in tqdm(dates):\n",
    "#     with open('./IEEE_news/df_03{}_news.pickle'.format(d), 'rb') as handle:\n",
    "#         dfs_news[d] = pickle.load(handle)\n",
    "#     with open('./IEEE_tweets/df_03{}_tweets.pickle'.format(d), 'rb') as handle:\n",
    "#         dfs_tweets[d] = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prepareation\n",
    "Tweets: extract unique original and retweets\n",
    "\n",
    "News: sentence level tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_tweets = {}\n",
    "def preDailyTweets(d):\n",
    "    df_tweets_retweets = dfs_tweets[d][dfs_tweets[d]['re_full_text']!=''].iloc[:,[0,2]].drop_duplicates('re_full_text')\n",
    "    df_tweets_retweets = df_tweets_retweets.rename(columns={'re_full_text':'full_text'})\n",
    "    df_original_tweets = dfs_tweets[d][dfs_tweets[d]['re_full_text']==''].iloc[:,[0,1]].drop_duplicates('full_text')\n",
    "    df_tweets = pd.concat([df_tweets_retweets,df_original_tweets]).reset_index()\n",
    "    tweets_text = df_tweets.full_text\n",
    "    tweets_token = apply_by_multiprocessing(tweets_text, processText, workers=32)\n",
    "    day_tweets = pd.DataFrame({'text':tweets_text,'tokens':tweets_token})\n",
    "    day_tweets.loc[:,'countTokens'] = [len(i) for i in day_tweets.tokens]\n",
    "    day_tweets = day_tweets[day_tweets.countTokens!=0]#delete 0 tokens row\n",
    "    return day_tweets\n",
    "\n",
    "# daily_tweets[d] = preDailyTweets(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['qanon', 'blast', 'past', 'gt', 'gt', 'b', 'f', 'p', 'gt', 'gt', 'q', 'gt', 'cue', 'gt', 'gt', 'clue', 'gt', 'gt', 'c', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'c', 'b', 'k', 'gt', 'gt', 'supreme', 'court', 'brett', 'kavanaugh', 'gt', 'gt', 'likes', 'beer', 'gt', 'gt', 'corona', 'gt', 'gt', 'c', 'r', 'n', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'p', 'gt', 'gt', 'f', 'gt', 'gt', 'p', 'f', 'gt', 'gt', 'f', 'p', 'gt', 'gt', 'false', 'pandemic', 'gt', 'gt']\n"
     ]
    }
   ],
   "source": [
    "print(daily_tweets[d][daily_tweets[d].countTokens==71].tokens[239699])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_news = {}\n",
    "def preDailyNews(d):\n",
    "    df_news = []\n",
    "    for i in dfs_news[d].loc[:,'text']: # sentence level\n",
    "        df_news.extend(sent_tokenize(i))\n",
    "    news_text = pd.Series(df_news).unique()\n",
    "    news_token = apply_by_multiprocessing(pd.Series(news_text), processText, workers=32)\n",
    "    news_countTokens = [len(i) for i in news_token]\n",
    "    day_news =  pd.DataFrame({'text':news_text,'tokens':news_token,'countTokens':news_countTokens})\n",
    "    day_news = day_news[day_news.countTokens!=0]\n",
    "    return day_news\n",
    "\n",
    "# daily_news[d] = preDailyNews(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The national animal - the tiger and the national bird - the peacock , go about their lives, unmindful of each other, at the Nagarhole National Park\\nThe picture was clicked by reader Arvind ramakrishnan\\nIf you have a picture and a story to share, send us the picture with your name and number at mybangaloremirror@timesgroup.com RELATED NEWS Happy Birthday Tiger Shroff: Disha Patani shares first dance video to... Mar 02, 2020\\nDisha Patani celebrates Valentine's Day sans Tiger Shroff Feb 14, 2020\\nWatch: Tiger Shroff aces a backflip in this new video Jan 16, 2020\\nGALLERIES View more photos Vidhan Soudha gets a disinfection treatment Prabhu Ganesan spotted shooting in Mumbai India vs Australia 3rd ODI Most Popular Most Read Most commented Covid-19 in Bengaluru: ‘Curfew’ that br... Covid-19 in Bengaluru: ‘Curfew’ that broke the chain COVID-19 in Bengaluru: ‘Stock up on foo... COVID-19 in Bengaluru: ‘Stock up on food items for a week’ Karnataka intensifies lockdown; no publ... Karnataka intensifies lockdown; no public transport services till March 31 Covid-19 in Bengaluru: Virtual Reality Water purifier service rivalry takes an... Water purifier service rivalry takes an ugly turn Covid-19 in Bengaluru: Bharatiya Janata... Covid-19 in Bengaluru: Bharatiya Janata DIDN’T Party COVID-19 in Bengaluru: ‘Stock up on foo... COVID-19 in Bengaluru: ‘Stock up on food items for a week’ COVID-19 in Bengaluru: ‘We’ll fix all W... COVID-19 in Bengaluru: ‘We’ll fix all WFH problems,’ says Deputy Chief Minister Dr CN Ashwath Narayan Real estate agents abduct ex-army man Finally, a road to Chrysalis School Want to fix Bengaluru?\""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_news[d][daily_news[d].countTokens==161].text[26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: 23  Tweets: 390616  News: 1468669\n"
     ]
    }
   ],
   "source": [
    "print(\"Date:\",d,' Tweets:',len(daily_tweets[d]),' News:',len(daily_news[d]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 1/7 [06:58<41:53, 418.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: 23  Tweets: 390616  News: 1468669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|██▊       | 2/7 [14:43<36:03, 432.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: 24  Tweets: 418034  News: 1608189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 43%|████▎     | 3/7 [22:30<29:31, 442.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: 25  Tweets: 377879  News: 1699977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 57%|█████▋    | 4/7 [30:31<22:43, 454.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: 26  Tweets: 386216  News: 1692317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|███████▏  | 5/7 [38:25<15:20, 460.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: 27  Tweets: 602484  News: 1578489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|████████▌ | 6/7 [43:36<06:55, 415.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: 28  Tweets: 327211  News: 957997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [47:23<00:00, 406.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: 29  Tweets: 123247  News: 616230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# for d in tqdm(dates):\n",
    "#     daily_tweets[d] = preDailyTweets(d)\n",
    "#     del(dfs_tweets[d])\n",
    "#     daily_news[d] = preDailyNews(d)\n",
    "#     del(dfs_news[d])\n",
    "#     print(\"Date:\",d,' Tweets:',len(daily_tweets[d]),' News:',len(daily_news[d]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of constructing daily_xxx[date], seperate df into individual files\n",
    "# with open('daily_tweets.pickle', 'wb') as handle:\n",
    "#     pickle.dump(daily_tweets, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# with open('daily_news.pickle', 'wb') as handle:\n",
    "#     pickle.dump(daily_news, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# # seperate into mulitiple smaller files\n",
    "# for d in dates:\n",
    "#     with open(f'./data_combined/ge20_topic20-15-1g/senTo_Tweets_03{d}.pickle', 'wb') as handle:\n",
    "#         pickle.dump(daily_tweets[d].iloc[:,1:], handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance Sampling News\n",
    "# num_tweets = df_topic_sent.groupby('day').apply(lambda x:np.sum([x[0],x[1],x[2]]))\n",
    "for d in dates:\n",
    "    daily_news[d] = daily_news[d].iloc[:,1:].sample(n=num_tweets[d], random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>countTokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>240646</th>\n",
       "      <td>[contrary, home, computers, allowed, work, fol...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514870</th>\n",
       "      <td>[act, section, title, united, states, code, he...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>650773</th>\n",
       "      <td>[going, put, time, period, greenberg, called, ...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40098</th>\n",
       "      <td>[even, talking, asian, heads, government, beca...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1123157</th>\n",
       "      <td>[big, test, coming, nbn, vodafone, closed, num...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617404</th>\n",
       "      <td>[veteran, village, kins, community, arizona, c...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>894661</th>\n",
       "      <td>[little, wonder, germany, announced, plans, us...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194548</th>\n",
       "      <td>[agency, authority, crack, prosecute, illegal,...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>683517</th>\n",
       "      <td>[image, andrew, echo, sign, free, email, alert...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362277</th>\n",
       "      <td>[group, steadily, pushed, forward, constructio...</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>388431 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    tokens  countTokens\n",
       "240646   [contrary, home, computers, allowed, work, fol...           12\n",
       "514870   [act, section, title, united, states, code, he...           10\n",
       "650773   [going, put, time, period, greenberg, called, ...           12\n",
       "40098    [even, talking, asian, heads, government, beca...            9\n",
       "1123157  [big, test, coming, nbn, vodafone, closed, num...           11\n",
       "...                                                    ...          ...\n",
       "617404   [veteran, village, kins, community, arizona, c...           22\n",
       "894661   [little, wonder, germany, announced, plans, us...           10\n",
       "194548   [agency, authority, crack, prosecute, illegal,...            8\n",
       "683517   [image, andrew, echo, sign, free, email, alert...           16\n",
       "362277   [group, steadily, pushed, forward, constructio...           52\n",
       "\n",
       "[388431 rows x 2 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_news[23]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel_all= LdaMulticore.load('./TopicModels/ldamodel_all.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140570"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###### DEPRECATED #######\n",
    "# Use LDA model id2word attr to rebuild the dictionary\n",
    "# dict_word2id = dict(ldamodel_all.id2word)\n",
    "# dict_word2id = dict(zip(dict_word2id.values(), dict_word2id.keys()))\n",
    "\n",
    "# def my_doc2bow(token):  # same as the corpus.dictionary.doc2bow()\n",
    "#     token2id = []\n",
    "#     for w in token:\n",
    "#         if w in dict_word2id.keys():\n",
    "#             token2id.append(dict_word2id[w])\n",
    "#     if len(token2id)>0:\n",
    "#         w_dict_count = pd.Series(token2id).value_counts().sort_index()\n",
    "#         doc2bow = tuple(zip(w_dict_count.keys(),w_dict_count.values))\n",
    "#         return doc2bow\n",
    "#     else:\n",
    "# #         print(token)\n",
    "#         return []\n",
    "\n",
    "# topic_mapping = [maxTopic(my_doc2bow(i)) for i in tqdm(tokens)] # use my_doc2bow instead of gensim.corora.dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictTopic(bow):\n",
    "    if len(bow)>0:\n",
    "        res = dict(ldamodel_all[bow]) # vectors of all topics\n",
    "        return res\n",
    "    \n",
    "def maxTopic(res):\n",
    "    if res is not None:\n",
    "#         res2id = dict(zip(res.values(),res.keys()))\n",
    "#         maxid = res2id[pd.Series(res).max()]\n",
    "        return pd.Series(res).sort_values(ascending=False).index[0]\n",
    "#         return maxid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#faster methods: use gensim.corpora.dictionary\n",
    "#Directly use the subset dictionary that used in the previous training of LDA model \n",
    "with open('./TopicModels/dictionary_all.pickle', 'rb') as handle:\n",
    "    subset_dictionary = pickle.load(handle)\n",
    "from gensim import corpora\n",
    "def mapTopics(tokens, df=False):\n",
    "    topic_weight,topic_mapping = [],[]\n",
    "    for i in tokens:\n",
    "        bow = subset_dictionary.doc2bow(i)\n",
    "        prob = predictTopic(bow)\n",
    "        topic_weight.append(prob)\n",
    "        topic_mapping.append(maxTopic(prob))\n",
    "    if df:\n",
    "        df_topics = pd.DataFrame({'tokens':tokens,'weightTopic':topic_weight,'maxTopic':topic_mapping})\n",
    "        df_topics.dropna(inplace=True)\n",
    "        return df_topics\n",
    "    else:\n",
    "        return topic_weight,topic_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [1:32:41<00:00, 794.43s/it]\n"
     ]
    }
   ],
   "source": [
    "# Map topics to tweets\n",
    "for d in tqdm(dates):\n",
    "    topic_weight,topic_mapping = mapTopics(daily_tweets[d].tokens)\n",
    "    daily_tweets[d].loc[:,'weightedTopic'] = topic_weight\n",
    "    daily_tweets[d].loc[:,'maxTopic'] = topic_mapping    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map合并结果保存\n",
    "# with open('daily_tweets.pickle', 'wb') as handle:\n",
    "#     pickle.dump(daily_tweets, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only tokens columns for tweets\n",
    "# for d in range(23,30):\n",
    "#     daily_tweets[d] = daily_tweets[d].loc[:,['tokens']]\n",
    "\n",
    "# with open('daily_tweets_tokens.pickle', 'wb') as handle:\n",
    "#     pickle.dump(daily_tweets, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [49:35<00:00, 425.06s/it]\n"
     ]
    }
   ],
   "source": [
    "# Map topics to News\n",
    "for d in tqdm(dates):\n",
    "    topic_weight,topic_mapping = mapTopics(daily_news[d].tokens)\n",
    "    daily_news[d].loc[:,'weightedTopic'] = topic_weight\n",
    "    daily_news[d].loc[:,'maxTopic'] = topic_mapping    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('daily_news_balanced.pickle', 'wb') as handle:\n",
    "#     pickle.dump(daily_news, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# tf.config.set_visible_devices([], 'GPU') #禁用GPU\n",
    "with open('../SentimentAnalysis/tokenizer.pickle', 'rb') as handle:\n",
    "    t = pickle.load(handle)\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "model_ge20 = load_model('../SentimentAnalysis/models/model_ge20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def text2Seq(tokens):\n",
    "    text_seq = t.texts_to_sequences(tokens)\n",
    "    return text_seq\n",
    "def padSeq(seq):\n",
    "    max_length = 21\n",
    "    text_pad = pad_sequences(seq, maxlen=max_length, padding='post')\n",
    "    return text_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [02:39<00:00, 22.74s/it]\n"
     ]
    }
   ],
   "source": [
    "# News\n",
    "df_sent = {}\n",
    "for d in tqdm(dates):\n",
    "    text_seq = text2Seq(daily_news[d].tokens)\n",
    "    text_pad = padSeq(text_seq)\n",
    "    pred_sent_prob_ge20 = model_ge20.predict(text_pad)\n",
    "    pred_sent_ge20 = [np.argmax(i) for i in pred_sent_prob_ge20]\n",
    "    df_sent[d] = pd.DataFrame(model_ge20.predict(text_pad),columns=['s1','s2','s3'])\n",
    "    df_sent[d].loc[:,'sent'] = pred_sent_ge20\n",
    "    # combine df_sent into daily_news\n",
    "    daily_news[d] = daily_news[d].reset_index(drop=True)\n",
    "    daily_news[d] = pd.concat([daily_news[d],df_sent[d]], axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tweets\n",
    "# Import the results from Google colab, combine into daily_tweets\n",
    "# with open('sent_tweets.pickle', 'rb') as handle:\n",
    "#     sent_tweets = pickle.load(handle)\n",
    "\n",
    "for d in dates:\n",
    "    daily_tweets[d] = daily_tweets[d].reset_index(drop=True)\n",
    "    daily_tweets[d] = pd.concat([daily_tweets[d],sent_tweets[d]], axis=1, sort=False)\n",
    "# Final daily_tweets: combined all the mappings, tokens, and original text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic + Topics\n",
    "Data aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data_combined/daily_tweets.pickle', 'rb') as handle:\n",
    "    daily_tweets = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>countTokens</th>\n",
       "      <th>weightedTopic</th>\n",
       "      <th>maxTopic</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@YSS_hardoi Indian Sages were, &amp;amp; at presen...</td>\n",
       "      <td>[indian, sages, amp, present, great, yogi, amp...</td>\n",
       "      <td>26</td>\n",
       "      <td>{1: 0.9567716}</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.044425</td>\n",
       "      <td>0.033047</td>\n",
       "      <td>0.922527</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>CORONA festival being celebrated in #Ahmedabad...</td>\n",
       "      <td>[corona, festival, celebrated, ahmedabad, stup...</td>\n",
       "      <td>6</td>\n",
       "      <td>{14: 0.84159744}</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.034268</td>\n",
       "      <td>0.315994</td>\n",
       "      <td>0.649738</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>'Go Corona' Masterstroke by Modiji today\\n\\nEn...</td>\n",
       "      <td>[corona, masterstroke, modiji, today, entire, ...</td>\n",
       "      <td>29</td>\n",
       "      <td>{1: 0.14526416, 8: 0.07074893, 12: 0.10902816,...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.298927</td>\n",
       "      <td>0.055879</td>\n",
       "      <td>0.645194</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Amongst all the corona virus chaos I am so hap...</td>\n",
       "      <td>[amongst, corona, virus, chaos, happy, share, ...</td>\n",
       "      <td>26</td>\n",
       "      <td>{0: 0.33327836, 3: 0.08759302, 16: 0.20877703,...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.388928</td>\n",
       "      <td>0.063052</td>\n",
       "      <td>0.548020</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>We request @narendramodi ji must take the guid...</td>\n",
       "      <td>[request, ji, must, take, guidance, great, sai...</td>\n",
       "      <td>14</td>\n",
       "      <td>{1: 0.9366297}</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.026764</td>\n",
       "      <td>0.424973</td>\n",
       "      <td>0.548263</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  \\\n",
       "2   @YSS_hardoi Indian Sages were, &amp; at presen...   \n",
       "22  CORONA festival being celebrated in #Ahmedabad...   \n",
       "26  'Go Corona' Masterstroke by Modiji today\\n\\nEn...   \n",
       "51  Amongst all the corona virus chaos I am so hap...   \n",
       "75  We request @narendramodi ji must take the guid...   \n",
       "\n",
       "                                               tokens  countTokens  \\\n",
       "2   [indian, sages, amp, present, great, yogi, amp...           26   \n",
       "22  [corona, festival, celebrated, ahmedabad, stup...            6   \n",
       "26  [corona, masterstroke, modiji, today, entire, ...           29   \n",
       "51  [amongst, corona, virus, chaos, happy, share, ...           26   \n",
       "75  [request, ji, must, take, guidance, great, sai...           14   \n",
       "\n",
       "                                        weightedTopic  maxTopic        s1  \\\n",
       "2                                      {1: 0.9567716}       1.0  0.044425   \n",
       "22                                   {14: 0.84159744}      14.0  0.034268   \n",
       "26  {1: 0.14526416, 8: 0.07074893, 12: 0.10902816,...      14.0  0.298927   \n",
       "51  {0: 0.33327836, 3: 0.08759302, 16: 0.20877703,...       0.0  0.388928   \n",
       "75                                     {1: 0.9366297}       1.0  0.026764   \n",
       "\n",
       "          s2        s3  sent  \n",
       "2   0.033047  0.922527     2  \n",
       "22  0.315994  0.649738     2  \n",
       "26  0.055879  0.645194     2  \n",
       "51  0.063052  0.548020     2  \n",
       "75  0.424973  0.548263     2  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_tweets[23][daily_tweets[23].sent==2].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:02<00:00,  2.68it/s]\n"
     ]
    }
   ],
   "source": [
    "# for each topics count up sentiment labels\n",
    "def getSentRes(dfs_daily):\n",
    "    dfs_topic_sent = {}\n",
    "    for d in tqdm(dates):\n",
    "        topic_sent = dfs_daily[d].groupby('maxTopic').apply(lambda x: pd.Series(list(x['sent'])).value_counts())\n",
    "        if isinstance(topic_sent, pd.Series): #23由于不明情况是df不是series\n",
    "            df_topic_sent = pd.concat([pd.DataFrame(topic_sent[(i,)]).T for i in range(20) if (i,) in topic_sent.keys()],sort=False).reset_index(drop=True)\n",
    "        else:\n",
    "            df_topic_sent = topic_sent.reset_index(drop=True)\n",
    "        #  df_topic_sent[[0,1,2]].max(axis=1)\n",
    "        df_topic_sent.loc[:,'TopSent'] = [np.argmax([df_topic_sent[0][i],df_topic_sent[1][i],df_topic_sent[2][i]]) for i in range(len(df_topic_sent))]\n",
    "        dfs_topic_sent[d] = df_topic_sent\n",
    "        dfs_topic_sent[d].loc[:,'day']=d\n",
    "    return dfs_topic_sent\n",
    "\n",
    "dfs_topic_sent_tweets =  getSentRes(daily_tweets)\n",
    "dfs_topic_sent_news = getSentRes(daily_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data_combined/dfs_topic_sent.pickle', 'rb') as handle:\n",
    "    dfs_topic_sent = pickle.load(handle)\n",
    "with open('./data_combined/dfs_topic_sent_news.pickle', 'rb') as handle:\n",
    "    dfs_topic_sent_news = pickle.load(handle)\n",
    "def aggSentRes(dfs_topic_sent):\n",
    "    df = pd.concat([dfs_topic_sent[d] for d in dates]).reset_index()\n",
    "    df = df.rename(columns={'index':'TopicID'})\n",
    "    df.TopicID = df.TopicID + 1\n",
    "    df.loc[:,'total'] = df[0]+df[1]+df[2]\n",
    "    df.loc[:,'TopicScore'] = (df[2]-df[0])/df['total']\n",
    "    return df.drop(['TopSent'],axis=1)\n",
    "df_topic_sent = aggSentRes(dfs_topic_sent)\n",
    "df_topic_sent_news = aggSentRes(dfs_topic_sent_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./data_combined/df_topic_sent.pickle', 'wb') as handle:\n",
    "#     pickle.dump(df_topic_sent, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# with open('./data_combined/df_topic_sent_news.pickle', 'wb') as handle:\n",
    "#     pickle.dump(df_topic_sent_news, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TopicID</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>day</th>\n",
       "      <th>total</th>\n",
       "      <th>TopicScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1309</td>\n",
       "      <td>977</td>\n",
       "      <td>304</td>\n",
       "      <td>23</td>\n",
       "      <td>2590</td>\n",
       "      <td>-0.388031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5966</td>\n",
       "      <td>1637</td>\n",
       "      <td>649</td>\n",
       "      <td>23</td>\n",
       "      <td>8252</td>\n",
       "      <td>-0.644329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>6659</td>\n",
       "      <td>1431</td>\n",
       "      <td>783</td>\n",
       "      <td>23</td>\n",
       "      <td>8873</td>\n",
       "      <td>-0.662234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>50795</td>\n",
       "      <td>13653</td>\n",
       "      <td>659</td>\n",
       "      <td>23</td>\n",
       "      <td>65107</td>\n",
       "      <td>-0.770055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>4789</td>\n",
       "      <td>1852</td>\n",
       "      <td>204</td>\n",
       "      <td>23</td>\n",
       "      <td>6845</td>\n",
       "      <td>-0.669832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>16</td>\n",
       "      <td>7343</td>\n",
       "      <td>1391</td>\n",
       "      <td>557</td>\n",
       "      <td>29</td>\n",
       "      <td>9291</td>\n",
       "      <td>-0.730384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>17</td>\n",
       "      <td>320</td>\n",
       "      <td>336</td>\n",
       "      <td>110</td>\n",
       "      <td>29</td>\n",
       "      <td>766</td>\n",
       "      <td>-0.274151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>18</td>\n",
       "      <td>458</td>\n",
       "      <td>281</td>\n",
       "      <td>59</td>\n",
       "      <td>29</td>\n",
       "      <td>798</td>\n",
       "      <td>-0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>19</td>\n",
       "      <td>389</td>\n",
       "      <td>545</td>\n",
       "      <td>31</td>\n",
       "      <td>29</td>\n",
       "      <td>965</td>\n",
       "      <td>-0.370984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>20</td>\n",
       "      <td>10517</td>\n",
       "      <td>5379</td>\n",
       "      <td>1580</td>\n",
       "      <td>29</td>\n",
       "      <td>17476</td>\n",
       "      <td>-0.511387</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>140 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     TopicID      0      1     2  day  total  TopicScore\n",
       "0          1   1309    977   304   23   2590   -0.388031\n",
       "1          2   5966   1637   649   23   8252   -0.644329\n",
       "2          3   6659   1431   783   23   8873   -0.662234\n",
       "3          4  50795  13653   659   23  65107   -0.770055\n",
       "4          5   4789   1852   204   23   6845   -0.669832\n",
       "..       ...    ...    ...   ...  ...    ...         ...\n",
       "135       16   7343   1391   557   29   9291   -0.730384\n",
       "136       17    320    336   110   29    766   -0.274151\n",
       "137       18    458    281    59   29    798   -0.500000\n",
       "138       19    389    545    31   29    965   -0.370984\n",
       "139       20  10517   5379  1580   29  17476   -0.511387\n",
       "\n",
       "[140 rows x 7 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [np.argmax([df_topic_sent[0][i],df_topic_sent[1][i],df_topic_sent[2][i]]) for i in range(len(df_topic_sent))]\n",
    "df_topic_sent_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
