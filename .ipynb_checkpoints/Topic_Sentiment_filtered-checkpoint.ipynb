{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU numbers: 32\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt  \n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "stopwordEn = stopwords.words('english')\n",
    "# from nltk.corpus import wordnet\n",
    "import pickle\n",
    "import multiprocessing as mp\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.test.utils import datapath\n",
    "import pyLDAvis.gensim\n",
    "import time\n",
    "\n",
    "print('CPU numbers:',mp.cpu_count())\n",
    "def _apply_df(args):\n",
    "    df, func, kwargs = args\n",
    "    return df.apply(func, **kwargs)\n",
    "def apply_by_multiprocessing(df, func, **kwargs):\n",
    "#     print(kwargs)\n",
    "    workers = kwargs.pop('workers')\n",
    "    pool = mp.Pool(processes=workers)\n",
    "    result = pool.map(_apply_df, [(d, func, kwargs) for d in np.array_split(df, workers)])\n",
    "    pool.close()\n",
    "    return pd.concat(list(result))\n",
    "#apply_by_multiprocessing(fullset['Text'], processText, workers=cores)\n",
    "# New Filter    \n",
    "def sub_ent(i,length):\n",
    "    p = float(i/length) #频率作为概率\n",
    "    logp = np.log2(p)\n",
    "    return -1 * p * logp\n",
    "def calc_ent(x):\n",
    "    if x is not None:\n",
    "        x_dict = pd.Series(x).value_counts()\n",
    "        return x_dict.apply(sub_ent, length=len(x)).sum()\n",
    "\n",
    "stopwordEn = stopwords.words('english')\n",
    "stopwordEn.extend(['amp'])\n",
    "whitelist = [\"n't\", \"not\", \"no\"]\n",
    "def prepareNewsSen(i,lemma=False, gram=1):\n",
    "    i = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b|@\\w+|#', '', i, flags=re.MULTILINE) #delete URL, # , and @xxx\n",
    "    if len(i) > 10: # delete tweet contains less than 5 characters\n",
    "        samples = int(len(i)* 0.1) # sampling 10% characters\n",
    "        random_c = np.random.choice(len(i),replace=False, size=samples if samples > 5 else 5) # at least 5 samples\n",
    "#         # if less than 50% sampled character is not English charater, delete the tweets \n",
    "        if sum([1 if (i[s] >= u'\\u0041' and i[s] <= u'\\u005a') or (i[s] >= u'\\u0061' and i[s] <= u'\\u007a')\\\n",
    "                or i[s].isspace() or i[s].isdigit() or i[s] in [' ','.',',','!'] else 0 for s in random_c]) > len(random_c)/2 :\n",
    "#             if detect(i)=='en': #slow but accuracte language detect\n",
    "                tokens = word_tokenize(i)\n",
    "                tokens = [lemmaWord(i.lower()) if lemma else i.lower() for i in tokens if (i.lower() not in stopwordEn or i.lower() in whitelist) and i.isalpha()]\n",
    "                if gram>1: tokens = [i for i in nltk.ngrams(tokens, gram)]\n",
    "                if len(tokens) > 4: # 2.0 text entropy for 4 different words\n",
    "                    entropy = calc_ent(tokens)\n",
    "                    if entropy < 20 or entropy > 2:\n",
    "                        return tokens #, entropy\n",
    "dates = range(19,31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [01:03<00:00,  5.26s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load the whole data, which may exceed memory\n",
    "# dfs_news, dfs_tweets={},{}\n",
    "# dates = range(19,31)\n",
    "# for d in tqdm(dates):\n",
    "#     with open('./IEEE_news/filtered_data/df_03{}_news_filtered.pickle'.format(d), 'rb') as handle:\n",
    "#         dfs_news[d] = pickle.load(handle)\n",
    "# #     with open('./IEEE_tweets/filtered_data/df_03{}_tweets_filtered.pickle'.format(d), 'rb') as handle:\n",
    "# #         dfs_tweets[d] = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prepareation\n",
    "Tweets: extract unique original and retweets\n",
    "\n",
    "News: sentence level tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preDailyNews(dfs,d):\n",
    "    news, published = [],[]\n",
    "    dfs_news[d] = dfs_news[d].reset_index(drop=True)\n",
    "    res = apply_by_multiprocessing(dfs[d].loc[:,'text'], sent_tokenize, workers=32)  # sentence level\n",
    "    # each sentence is corresponding to their published dates\n",
    "    for i in range(len(res)):\n",
    "        news.extend(res[i])\n",
    "        published.extend(np.array([dfs[d].loc[i,'published']]*len(res[i])))\n",
    "    del dfs[d]\n",
    "    df_news = pd.DataFrame({'published':published,'full_text':news}) #build a dataframe for each news sentence and dates\n",
    "    del news, published \n",
    "    \n",
    "    df_news.loc[:,'tokens'] = apply_by_multiprocessing(df_news['full_text'], prepareNewsSen, workers=32) # tokenize. filter\n",
    "#     del df_news['full_text'] # delete original sentence text\n",
    "    df_news = df_news.dropna() # delte none tokens\n",
    "#     https://mkyong.com/python/python-md5-hashing-example/\n",
    "    df_news.loc[:,'tokens_hash'] = [''.join(i) for i in df_news.tokens] # make token list hashable\n",
    "    df_news = df_news.drop_duplicates(['tokens_hash']) # delete repetitive tokens\n",
    "    df_news = df_news.iloc[:,:-1] # delete tokens string\n",
    "    df_news = df_news.reset_index(drop=True)\n",
    "    df_news.loc[:,'countTokens'] = [len(i) for i in df_news.tokens]\n",
    "    return df_news "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 1/12 [01:04<11:47, 64.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: 19  Tweets: 42076  News: 179990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|█▋        | 2/12 [02:23<11:28, 68.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: 20  Tweets: 169351  News: 176129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▌       | 3/12 [03:04<09:04, 60.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: 21  Tweets: 185271  News: 99592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███▎      | 4/12 [03:42<07:09, 53.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: 22  Tweets: 200616  News: 85771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|████▏     | 5/12 [04:48<06:40, 57.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: 23  Tweets: 208484  News: 160140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 6/12 [05:54<06:00, 60.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: 24  Tweets: 204924  News: 175879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 58%|█████▊    | 7/12 [07:04<05:14, 62.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: 25  Tweets: 184380  News: 186965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████▋   | 8/12 [08:18<04:25, 66.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: 26  Tweets: 188601  News: 183878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▌  | 9/12 [09:34<03:27, 69.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: 27  Tweets: 295009  News: 173192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|████████▎ | 10/12 [10:17<02:02, 61.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: 28  Tweets: 163046  News: 105143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|█████████▏| 11/12 [10:44<00:50, 50.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: 29  Tweets: 62255  News: 65656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [10:53<00:00, 54.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: 30  Tweets: 102618  News: 519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Instead of loading the whole data, load and process data of each day\n",
    "# Use sampling \n",
    "dfs_news, dfs_tweets={},{}\n",
    "daily_news, daily_tweets={},{}\n",
    "for d in tqdm(dates):\n",
    "    # News\n",
    "    with open('./IEEE_news/filtered_data/df_03{}_news_filtered.pickle'.format(d), 'rb') as handle:\n",
    "        dfs_news[d] = pickle.load(handle)\n",
    "    # samples\n",
    "    dfs_news[d] = dfs_news[d].sample(n=int(len(dfs_news[d])* 0.1), random_state=1)\n",
    "    dfs_news[d] = dfs_news[d].reset_index(drop=True)\n",
    "    daily_news[d] = preDailyNews(dfs_news,d)\n",
    "\n",
    "    # Tweets\n",
    "    with open('./IEEE_tweets/filtered_data/df_03{}_tweets_filtered.pickle'.format(d), 'rb') as handle:\n",
    "        dfs_tweets[d] = pickle.load(handle)\n",
    "    daily_tweets[d] = dfs_tweets[d].sample(n=int(len(dfs_tweets[d])* 0.6847363488), random_state=1)\n",
    "    del dfs_tweets[d] \n",
    "    print(\"Date:\",d,' Tweets:',len(daily_tweets[d]),' News:',len(daily_news[d]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save daily results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of constructing daily_xxx[date], seperate df into individual files\n",
    "# with open('./filtered_daily/subset_1_filtered_daily_tweets.pickle', 'wb') as handle:\n",
    "#     pickle.dump(daily_tweets, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# with open('./filtered_daily/subset_1_filtered_daily_news.pickle', 'wb') as handle:\n",
    "#     pickle.dump(daily_news, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# # seperate into mulitiple smaller files\n",
    "# for d in dates:\n",
    "#     with open(f'./data_combined/ge20_topic20-15-1g/senTo_Tweets_03{d}.pickle', 'wb') as handle:\n",
    "#         pickle.dump(daily_tweets[d].iloc[:,1:], handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read daily results\n",
    "with open('./filtered_daily/subset_1_filtered_daily_tweets.pickle', 'rb') as handle:\n",
    "    daily_tweets =  pickle.load(handle)\n",
    "with open('./filtered_daily/subset_1_filtered_daily_news.pickle', 'rb') as handle:\n",
    "    daily_news =  pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top Sentence for each topic\n",
    "input: daily_tweets/ daily_news\n",
    "\n",
    "output: TopTweets.xlsx , TopNews.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get top tweets of each topic\n",
    "def getTopSent(df):\n",
    "    dfs_res = {}\n",
    "    for d in tqdm(dates):\n",
    "        max_topic, top_tokens, top_text = {},{},{}\n",
    "#         for i in range(len(df[d].sample(n=1000,random_state=1))):\n",
    "        for i in range(len(df[d])):\n",
    "            maxTopic = df[d].maxTopic[i]\n",
    "            if df[d].weightedTopic[i] is not None:\n",
    "                TopicWeight = df[d].weightedTopic[i][maxTopic]\n",
    "                if maxTopic in max_topic:\n",
    "                    if TopicWeight > max_topic[maxTopic]:\n",
    "                        max_topic[maxTopic] = TopicWeight\n",
    "                        top_tokens[maxTopic] = df[d].tokens[i]\n",
    "                        top_text[maxTopic] = df[d].full_text[i]\n",
    "\n",
    "                else:\n",
    "                    max_topic[maxTopic] = TopicWeight\n",
    "                    top_tokens[maxTopic] = df[d].tokens[i]\n",
    "                    top_text[maxTopic] = df[d].full_text[i]\n",
    "        dfs_res[d] = pd.DataFrame({'Max Topic Weight':max_topic, 'tokens':top_tokens, 'text':top_text}).sort_index()\n",
    "    dfs_res = pd.concat(dfs_res).reset_index()\n",
    "    dfs_res = dfs_res.rename(columns={\"level_0\":\"date\",\"level_1\":\"topic\"})\n",
    "    dfs_res = dfs_res.sort_values(by='topic')\n",
    "    return dfs_res\n",
    "\n",
    "getTopSent(daily_tweets).to_excel('./filtered_daily/TopTweets_10x.xlsx',index=False)\n",
    "getTopSent(daily_news).to_excel('./filtered_daily/TopNews_10x.xlsx',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179990 179985\n",
      "176129 176126\n",
      "99592 99596\n",
      "85771 85771\n",
      "160140 160139\n",
      "175879 175881\n",
      "186965 186974\n",
      "183878 183875\n",
      "173192 173187\n",
      "105143 105143\n",
      "65656 65654\n",
      "519 519\n"
     ]
    }
   ],
   "source": [
    "# 重新map ，map前后rows应该不变\n",
    "for d in dates:\n",
    "    print(len(daily_news[d]), len(daily_news_text[d]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:30<00:00,  9.07s/it]\n"
     ]
    }
   ],
   "source": [
    "# get top news of each topic\n",
    "def getTopSent(df):\n",
    "    dfs_res = {}\n",
    "    for d in tqdm(dates): # only 10 news samples in Mar 30 \n",
    "        max_topic, top_tokens, top_text = {},{},{}\n",
    "#         for i in range(len(df[d].sample(n=1000,random_state=1))):\n",
    "        for i in range(len(df[d])):\n",
    "            maxTopic = df[d].maxTopic[i]\n",
    "            if df[d].weightedTopic[i] is not None:\n",
    "                TopicWeight = df[d].weightedTopic[i][maxTopic]\n",
    "                if maxTopic in max_topic:\n",
    "                    if TopicWeight > max_topic[maxTopic]:\n",
    "                        max_topic[maxTopic] = TopicWeight\n",
    "                        top_tokens[maxTopic] = df[d].tokens[i]\n",
    "                        top_text[maxTopic] = df[d].full_text[i]\n",
    "\n",
    "                else:\n",
    "                    max_topic[maxTopic] = TopicWeight\n",
    "                    top_tokens[maxTopic] = df[d].tokens[i]\n",
    "                    top_text[maxTopic] = df[d].full_text[i]\n",
    "        dfs_res[d] = pd.DataFrame({'Max Topic Weight':max_topic, 'tokens':top_tokens}).sort_index()\n",
    "    return dfs_res\n",
    "dfs_res = getTopSent(daily_news)\n",
    "dfs_res = pd.concat(dfs_res).reset_index()\n",
    "dfs_res = dfs_res.rename(columns={\"level_0\":\"date\",\"level_1\":\"topic\"})\n",
    "dfs_res = dfs_res.sort_values(by='topic')\n",
    "dfs_res.to_excel('./filtered_daily/TopNews_10x.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>published</th>\n",
       "      <th>tokens</th>\n",
       "      <th>countTokens</th>\n",
       "      <th>weightedTopic</th>\n",
       "      <th>maxTopic</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-03-23 10:33:00+02:00</td>\n",
       "      <td>[share, single, biggest, sporting, event, worl...</td>\n",
       "      <td>35</td>\n",
       "      <td>{3: 0.02075416, 13: 0.95348716}</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.990831</td>\n",
       "      <td>0.005356</td>\n",
       "      <td>0.003812</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-03-23 10:33:00+02:00</td>\n",
       "      <td>[yet, coronavirus, raging, number, casualties,...</td>\n",
       "      <td>11</td>\n",
       "      <td>{2: 0.34563458, 3: 0.16055493, 8: 0.12975445, ...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.485704</td>\n",
       "      <td>0.491525</td>\n",
       "      <td>0.022771</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-03-23 10:33:00+02:00</td>\n",
       "      <td>[boxing, fans, know, scheduled, fights, either...</td>\n",
       "      <td>19</td>\n",
       "      <td>{2: 0.677087, 4: 0.09893691, 5: 0.18144292}</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.513363</td>\n",
       "      <td>0.426123</td>\n",
       "      <td>0.060513</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-03-23 10:33:00+02:00</td>\n",
       "      <td>[could, olympics, put, hold, full, year]</td>\n",
       "      <td>6</td>\n",
       "      <td>{9: 0.40470576, 16: 0.46669933}</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.855838</td>\n",
       "      <td>0.134548</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-03-23 10:33:00+02:00</td>\n",
       "      <td>[far, games, set, begin, july, end, august, to...</td>\n",
       "      <td>15</td>\n",
       "      <td>{2: 0.62782717, 9: 0.3158965}</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.425581</td>\n",
       "      <td>0.323854</td>\n",
       "      <td>0.250565</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160135</th>\n",
       "      <td>2020-03-23 18:53:00+02:00</td>\n",
       "      <td>[got, hymn, books, handed, residents]</td>\n",
       "      <td>5</td>\n",
       "      <td>{0: 0.010011019, 1: 0.010011019, 2: 0.01001101...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.122229</td>\n",
       "      <td>0.641372</td>\n",
       "      <td>0.236400</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160136</th>\n",
       "      <td>2020-03-23 18:53:00+02:00</td>\n",
       "      <td>[residents, complex, told, precaution, due, co...</td>\n",
       "      <td>12</td>\n",
       "      <td>{2: 0.111887515, 9: 0.2882859, 12: 0.53293747}</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.993098</td>\n",
       "      <td>0.006680</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160137</th>\n",
       "      <td>2020-03-23 18:53:00+02:00</td>\n",
       "      <td>[audrey, bigley, plays, crowd, balcony, freema...</td>\n",
       "      <td>11</td>\n",
       "      <td>{13: 0.8093004, 17: 0.10037672}</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.262991</td>\n",
       "      <td>0.683466</td>\n",
       "      <td>0.053543</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160138</th>\n",
       "      <td>2020-03-23 18:53:00+02:00</td>\n",
       "      <td>[picture, audrey, bigley, mrs, bigley, lives, ...</td>\n",
       "      <td>17</td>\n",
       "      <td>{13: 0.68955386, 19: 0.25406122}</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.218320</td>\n",
       "      <td>0.402417</td>\n",
       "      <td>0.379263</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160139</th>\n",
       "      <td>2020-03-23 18:53:00+02:00</td>\n",
       "      <td>[added, social, afternoon, quite, often, play,...</td>\n",
       "      <td>14</td>\n",
       "      <td>{5: 0.32599095, 13: 0.6138524}</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.695617</td>\n",
       "      <td>0.221613</td>\n",
       "      <td>0.082769</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>160140 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       published  \\\n",
       "0      2020-03-23 10:33:00+02:00   \n",
       "1      2020-03-23 10:33:00+02:00   \n",
       "2      2020-03-23 10:33:00+02:00   \n",
       "3      2020-03-23 10:33:00+02:00   \n",
       "4      2020-03-23 10:33:00+02:00   \n",
       "...                          ...   \n",
       "160135 2020-03-23 18:53:00+02:00   \n",
       "160136 2020-03-23 18:53:00+02:00   \n",
       "160137 2020-03-23 18:53:00+02:00   \n",
       "160138 2020-03-23 18:53:00+02:00   \n",
       "160139 2020-03-23 18:53:00+02:00   \n",
       "\n",
       "                                                   tokens  countTokens  \\\n",
       "0       [share, single, biggest, sporting, event, worl...           35   \n",
       "1       [yet, coronavirus, raging, number, casualties,...           11   \n",
       "2       [boxing, fans, know, scheduled, fights, either...           19   \n",
       "3                [could, olympics, put, hold, full, year]            6   \n",
       "4       [far, games, set, begin, july, end, august, to...           15   \n",
       "...                                                   ...          ...   \n",
       "160135              [got, hymn, books, handed, residents]            5   \n",
       "160136  [residents, complex, told, precaution, due, co...           12   \n",
       "160137  [audrey, bigley, plays, crowd, balcony, freema...           11   \n",
       "160138  [picture, audrey, bigley, mrs, bigley, lives, ...           17   \n",
       "160139  [added, social, afternoon, quite, often, play,...           14   \n",
       "\n",
       "                                            weightedTopic  maxTopic        s1  \\\n",
       "0                         {3: 0.02075416, 13: 0.95348716}      13.0  0.990831   \n",
       "1       {2: 0.34563458, 3: 0.16055493, 8: 0.12975445, ...       2.0  0.485704   \n",
       "2             {2: 0.677087, 4: 0.09893691, 5: 0.18144292}       2.0  0.513363   \n",
       "3                         {9: 0.40470576, 16: 0.46669933}      16.0  0.855838   \n",
       "4                           {2: 0.62782717, 9: 0.3158965}       2.0  0.425581   \n",
       "...                                                   ...       ...       ...   \n",
       "160135  {0: 0.010011019, 1: 0.010011019, 2: 0.01001101...      12.0  0.122229   \n",
       "160136     {2: 0.111887515, 9: 0.2882859, 12: 0.53293747}      12.0  0.993098   \n",
       "160137                    {13: 0.8093004, 17: 0.10037672}      13.0  0.262991   \n",
       "160138                   {13: 0.68955386, 19: 0.25406122}      13.0  0.218320   \n",
       "160139                     {5: 0.32599095, 13: 0.6138524}      13.0  0.695617   \n",
       "\n",
       "              s2        s3  sent  \n",
       "0       0.005356  0.003812     0  \n",
       "1       0.491525  0.022771     1  \n",
       "2       0.426123  0.060513     0  \n",
       "3       0.134548  0.009614     0  \n",
       "4       0.323854  0.250565     0  \n",
       "...          ...       ...   ...  \n",
       "160135  0.641372  0.236400     1  \n",
       "160136  0.006680  0.000222     0  \n",
       "160137  0.683466  0.053543     1  \n",
       "160138  0.402417  0.379263     1  \n",
       "160139  0.221613  0.082769     0  \n",
       "\n",
       "[160140 rows x 9 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_news[23]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Mapping\n",
    "\n",
    ".get_document_topics gives slightly different values evertime it is called\n",
    "https://github.com/RaRe-Technologies/gensim/issues/591"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel_all= LdaMulticore.load('./TopicModels/ldamodel_all_filtered.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictTopic(bow):\n",
    "    if len(bow)>0:\n",
    "        res = dict(ldamodel_all[bow]) # vectors of all topics\n",
    "        return res\n",
    "    \n",
    "def maxTopic(res):\n",
    "    if res is not None:\n",
    "#         res2id = dict(zip(res.values(),res.keys()))\n",
    "#         maxid = res2id[pd.Series(res).max()]\n",
    "        return pd.Series(res).sort_values(ascending=False).index[0]\n",
    "#         return maxid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#faster methods: use gensim.corpora.dictionary\n",
    "#Directly use the subset dictionary that used in the previous training of LDA model \n",
    "with open('./TopicModels/dictionary_all_filtered.pickle', 'rb') as handle:\n",
    "    subset_dictionary = pickle.load(handle)\n",
    "from gensim import corpora\n",
    "def mapTopics(tokens, df=False):\n",
    "    topic_weight,topic_mapping = [],[]\n",
    "    for i in tokens:\n",
    "        bow = subset_dictionary.doc2bow(i)\n",
    "        prob = predictTopic(bow)\n",
    "        topic_weight.append(prob)\n",
    "        topic_mapping.append(maxTopic(prob))\n",
    "    if df:\n",
    "        df_topics = pd.DataFrame({'tokens':tokens,'weightTopic':topic_weight,'maxTopic':topic_mapping})\n",
    "        df_topics.dropna(inplace=True)\n",
    "        return df_topics\n",
    "    else:\n",
    "        return topic_weight,topic_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [38:25<00:00, 192.16s/it]\n"
     ]
    }
   ],
   "source": [
    "# Map topics to tweets\n",
    "for d in tqdm(dates):\n",
    "    topic_weight,topic_mapping = mapTopics(daily_tweets[d].tokens)\n",
    "    daily_tweets[d].loc[:,'weightedTopic'] = topic_weight\n",
    "    daily_tweets[d].loc[:,'maxTopic'] = topic_mapping    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [30:52<00:00, 154.40s/it]\n"
     ]
    }
   ],
   "source": [
    "# Map topics to News\n",
    "for d in tqdm(dates):\n",
    "    topic_weight,topic_mapping = mapTopics(daily_news[d].tokens)\n",
    "    daily_news[d].loc[:,'weightedTopic'] = topic_weight\n",
    "    daily_news[d].loc[:,'maxTopic'] = topic_mapping    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "del ldamodel_all,subset_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# tf.config.set_visible_devices([], 'GPU') #禁用GPU\n",
    "with open('../SentimentAnalysis/tokenizer.pickle', 'rb') as handle:\n",
    "    t = pickle.load(handle)\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "model_ge20 = load_model('../SentimentAnalysis/models/model_ge20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def text2Seq(tokens):\n",
    "    text_seq = t.texts_to_sequences(tokens)\n",
    "    return text_seq\n",
    "def padSeq(seq):\n",
    "    max_length = 21\n",
    "    text_pad = pad_sequences(seq, maxlen=max_length, padding='post')\n",
    "    return text_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapSentiment(df,d):\n",
    "    text_seq = text2Seq(df[d].tokens)\n",
    "    text_pad = padSeq(text_seq)\n",
    "    pred_sent_prob_ge20 = model_ge20.predict(text_pad)\n",
    "    pred_sent_ge20 = [np.argmax(i) for i in pred_sent_prob_ge20]\n",
    "    df_sent = pd.DataFrame(model_ge20.predict(text_pad),columns=['s1','s2','s3'])\n",
    "    df_sent.loc[:,'sent'] = pred_sent_ge20\n",
    "    # combine df_sent into daily_news\n",
    "    df[d] = df[d].reset_index(drop=True)\n",
    "    df[d] = pd.concat([df[d],df_sent], axis=1, sort=False)\n",
    "    return df[d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [02:07<00:00, 10.66s/it]\n"
     ]
    }
   ],
   "source": [
    "# News\n",
    "for d in tqdm(dates):\n",
    "#     text_seq = text2Seq(daily_news[d].tokens)\n",
    "#     text_pad = padSeq(text_seq)\n",
    "#     pred_sent_prob_ge20 = model_ge20.predict(text_pad)\n",
    "#     pred_sent_ge20 = [np.argmax(i) for i in pred_sent_prob_ge20]\n",
    "#     df_sent[d] = pd.DataFrame(model_ge20.predict(text_pad),columns=['s1','s2','s3'])\n",
    "#     df_sent[d].loc[:,'sent'] = pred_sent_ge20\n",
    "#     # combine df_sent into daily_news\n",
    "#     daily_news[d] = daily_news[d].reset_index(drop=True)\n",
    "#     daily_news[d] = pd.concat([daily_news[d],df_sent[d]], axis=1, sort=False)\n",
    "    daily_news[d] = mapSentiment(daily_news,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [03:05<00:00, 15.43s/it]\n"
     ]
    }
   ],
   "source": [
    "#Tweets\n",
    "for d in tqdm(dates):\n",
    "    daily_tweets[d] = mapSentiment(daily_tweets,d)\n",
    "    \n",
    "# Final daily_tweets: combined all the mappings, tokens, and original text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic + Topics\n",
    "Data aggregation\n",
    "\n",
    "input: daily_tweets, daily_news\n",
    "\n",
    "output: df_topic_sent_tweets, df_topic_sent_news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12-hour aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:03<00:00,  3.57it/s]\n",
      "100%|██████████| 11/11 [00:02<00:00,  4.67it/s]\n"
     ]
    }
   ],
   "source": [
    "dates = range(19,30)\n",
    "def getHourSpan(dfs_daily,date_col='created_at'):\n",
    "    timepoints = ['00:00:00','12:00:00','23:59:59']\n",
    "    dfs_topic_sent = {}\n",
    "    time_n = 0\n",
    "    for d in tqdm(dates):\n",
    "        dfs_daily[d][date_col] = pd.to_datetime(dfs_daily[d][date_col], utc=True)\n",
    "        for t in range(len(timepoints)-1):\n",
    "            sub_df = dfs_daily[d][(dfs_daily[d][date_col] > f'2020-03-{d} {timepoints[t]}') & (dfs_daily[d][date_col] < f'2020-03-{d} {timepoints[t+1]}')]\n",
    "            topic_sent = sub_df.groupby('maxTopic').apply(lambda x: pd.Series(list(x['sent'])).value_counts())\n",
    "            if isinstance(topic_sent, pd.Series): \n",
    "                df_topic_sent = pd.concat([pd.DataFrame(topic_sent[(i,)]).T for i in range(20) if (i,) in topic_sent.keys()],sort=False).reset_index(drop=True).fillna(0)\n",
    "            else:\n",
    "                df_topic_sent = topic_sent.reset_index(drop=True).fillna(0)#23由于不明情况是df不是series\n",
    "            for i in range(3): # in case some data block doesn't have all 0,1,2 sentiments\n",
    "                if i not in df_topic_sent.columns:\n",
    "                    df_topic_sent[i]= 0\n",
    "            df_topic_sent.loc[:,'TopSent'] = [np.argmax([df_topic_sent[0][i],df_topic_sent[1][i],df_topic_sent[2][i]]) for i in range(len(df_topic_sent))]\n",
    "            df_topic_sent.loc[:,'timeSpan'] = [time_n]\n",
    "            df_topic_sent.loc[:,'startTime'] = [f\"2020-03{d} {timepoints[t]}\"]\n",
    "            dfs_topic_sent[time_n] = df_topic_sent\n",
    "            time_n += 1\n",
    "            \n",
    "    df = pd.concat([dfs_topic_sent[n] for n in range(time_n)]).reset_index()\n",
    "    df = df.rename(columns={'index':'TopicID'})\n",
    "    df.TopicID = df.TopicID + 1\n",
    "    df.loc[:,'total'] = df[0]+df[1]+df[2]\n",
    "    df.loc[:,'TopicScore'] = (df[2]-df[0])/df['total']\n",
    "    df = df.fillna(0)\n",
    "    return df.drop(['TopSent'],axis=1)\n",
    "df_topic_sent_tweets = getHourSpan(daily_tweets) \n",
    "df_topic_sent_news = getHourSpan(daily_news,'published') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TopicID</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>timeSpan</th>\n",
       "      <th>startTime</th>\n",
       "      <th>total</th>\n",
       "      <th>TopicScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-0319 00:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>733.0</td>\n",
       "      <td>308.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-0319 12:00:00</td>\n",
       "      <td>1194.0</td>\n",
       "      <td>-0.485762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>116.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-0319 12:00:00</td>\n",
       "      <td>277.0</td>\n",
       "      <td>-0.064982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>902.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-0319 12:00:00</td>\n",
       "      <td>1166.0</td>\n",
       "      <td>-0.704974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1325.0</td>\n",
       "      <td>294.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-0319 12:00:00</td>\n",
       "      <td>1656.0</td>\n",
       "      <td>-0.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>17</td>\n",
       "      <td>1083.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>20</td>\n",
       "      <td>2020-0329 00:00:00</td>\n",
       "      <td>1337.0</td>\n",
       "      <td>-0.792072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>18</td>\n",
       "      <td>68.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>20</td>\n",
       "      <td>2020-0329 00:00:00</td>\n",
       "      <td>138.0</td>\n",
       "      <td>-0.405797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>19</td>\n",
       "      <td>4041.0</td>\n",
       "      <td>284.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>20</td>\n",
       "      <td>2020-0329 00:00:00</td>\n",
       "      <td>4406.0</td>\n",
       "      <td>-0.898774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>20</td>\n",
       "      <td>1737.0</td>\n",
       "      <td>497.0</td>\n",
       "      <td>584.0</td>\n",
       "      <td>20</td>\n",
       "      <td>2020-0329 00:00:00</td>\n",
       "      <td>2818.0</td>\n",
       "      <td>-0.409155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21</td>\n",
       "      <td>2020-0329 12:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>383 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     TopicID       0      1      2  timeSpan           startTime   total  \\\n",
       "0          1     0.0    0.0    0.0         0  2020-0319 00:00:00     0.0   \n",
       "1          1   733.0  308.0  153.0         1  2020-0319 12:00:00  1194.0   \n",
       "2          2   116.0   63.0   98.0         1  2020-0319 12:00:00   277.0   \n",
       "3          3   902.0  184.0   80.0         1  2020-0319 12:00:00  1166.0   \n",
       "4          4  1325.0  294.0   37.0         1  2020-0319 12:00:00  1656.0   \n",
       "..       ...     ...    ...    ...       ...                 ...     ...   \n",
       "378       17  1083.0  230.0   24.0        20  2020-0329 00:00:00  1337.0   \n",
       "379       18    68.0   58.0   12.0        20  2020-0329 00:00:00   138.0   \n",
       "380       19  4041.0  284.0   81.0        20  2020-0329 00:00:00  4406.0   \n",
       "381       20  1737.0  497.0  584.0        20  2020-0329 00:00:00  2818.0   \n",
       "382        1     0.0    0.0    0.0        21  2020-0329 12:00:00     0.0   \n",
       "\n",
       "     TopicScore  \n",
       "0      0.000000  \n",
       "1     -0.485762  \n",
       "2     -0.064982  \n",
       "3     -0.704974  \n",
       "4     -0.777778  \n",
       "..          ...  \n",
       "378   -0.792072  \n",
       "379   -0.405797  \n",
       "380   -0.898774  \n",
       "381   -0.409155  \n",
       "382    0.000000  \n",
       "\n",
       "[383 rows x 8 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_topic_sent_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deal with missing data in couple of time span\n",
    "def fillEmptySpan(df_daily):\n",
    "    zeroTopics = df_daily.groupby('timeSpan')['TopicID'].count()\n",
    "    zeroTopics = zeroTopics[zeroTopics<20]\n",
    "    for t in list(zeroTopics.index):\n",
    "        timestamp = list(df_daily[df_daily['timeSpan']==t]['startTime'])[0]\n",
    "        for topic in [i for i in range(1,21) if i not in df_daily[df_daily.timeSpan==t].TopicID.unique()]:\n",
    "            df_daily = df_daily.append({'timeSpan': t,'startTime':timestamp, 'TopicID':topic}, ignore_index=True)\n",
    "    df_daily = df_daily.fillna(0)\n",
    "    df_daily = df_daily.sort_values(by='startTime')\n",
    "    df_daily = df_daily.reset_index(drop=True)\n",
    "    for i in [0,1,2,'total']:\n",
    "         df_daily[i] = df_daily[i].astype('int64')\n",
    "    return df_daily\n",
    "df_topic_sent_news = fillEmptySpan(df_topic_sent_news)\n",
    "df_topic_sent_tweets = fillEmptySpan(df_topic_sent_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TopicID</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>timeSpan</th>\n",
       "      <th>startTime</th>\n",
       "      <th>total</th>\n",
       "      <th>TopicScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>237</td>\n",
       "      <td>156</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-0319 00:00:00</td>\n",
       "      <td>422</td>\n",
       "      <td>-0.492891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>863</td>\n",
       "      <td>215</td>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-0319 00:00:00</td>\n",
       "      <td>1252</td>\n",
       "      <td>-0.550319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19</td>\n",
       "      <td>2720</td>\n",
       "      <td>236</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-0319 00:00:00</td>\n",
       "      <td>3005</td>\n",
       "      <td>-0.888852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>95</td>\n",
       "      <td>77</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-0319 00:00:00</td>\n",
       "      <td>190</td>\n",
       "      <td>-0.405263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>9227</td>\n",
       "      <td>2913</td>\n",
       "      <td>93</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-0319 00:00:00</td>\n",
       "      <td>12233</td>\n",
       "      <td>-0.746669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>2020-0329 12:00:00</td>\n",
       "      <td>37</td>\n",
       "      <td>-0.405405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>1</td>\n",
       "      <td>111</td>\n",
       "      <td>73</td>\n",
       "      <td>14</td>\n",
       "      <td>21</td>\n",
       "      <td>2020-0329 12:00:00</td>\n",
       "      <td>198</td>\n",
       "      <td>-0.489899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>19</td>\n",
       "      <td>940</td>\n",
       "      <td>77</td>\n",
       "      <td>14</td>\n",
       "      <td>21</td>\n",
       "      <td>2020-0329 12:00:00</td>\n",
       "      <td>1031</td>\n",
       "      <td>-0.898157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>9</td>\n",
       "      <td>132</td>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>2020-0329 12:00:00</td>\n",
       "      <td>170</td>\n",
       "      <td>-0.758824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>20</td>\n",
       "      <td>301</td>\n",
       "      <td>80</td>\n",
       "      <td>69</td>\n",
       "      <td>21</td>\n",
       "      <td>2020-0329 12:00:00</td>\n",
       "      <td>450</td>\n",
       "      <td>-0.515556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>440 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     TopicID     0     1    2  timeSpan           startTime  total  TopicScore\n",
       "0          1   237   156   29         0  2020-0319 00:00:00    422   -0.492891\n",
       "1         20   863   215  174         0  2020-0319 00:00:00   1252   -0.550319\n",
       "2         19  2720   236   49         0  2020-0319 00:00:00   3005   -0.888852\n",
       "3         18    95    77   18         0  2020-0319 00:00:00    190   -0.405263\n",
       "4         17  9227  2913   93         0  2020-0319 00:00:00  12233   -0.746669\n",
       "..       ...   ...   ...  ...       ...                 ...    ...         ...\n",
       "435        2    17    18    2        21  2020-0329 12:00:00     37   -0.405405\n",
       "436        1   111    73   14        21  2020-0329 12:00:00    198   -0.489899\n",
       "437       19   940    77   14        21  2020-0329 12:00:00   1031   -0.898157\n",
       "438        9   132    35    3        21  2020-0329 12:00:00    170   -0.758824\n",
       "439       20   301    80   69        21  2020-0329 12:00:00    450   -0.515556\n",
       "\n",
       "[440 rows x 8 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_topic_sent_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./filtered_daily/df_topic_sent_tweets_filtered_subset_1_12hrs.pickle', 'wb') as handle:\n",
    "    pickle.dump(df_topic_sent_tweets, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('./filtered_daily/df_topic_sent_news_filtered_subset_1_12hrs.pickle', 'wb') as handle:\n",
    "    pickle.dump(df_topic_sent_news, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daily Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:00<00:00, 11.23it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 16.52it/s]\n"
     ]
    }
   ],
   "source": [
    "# for each topics count up sentiment labels\n",
    "dates = range(19,30) # Only few of news in Mar 30, so we only use [19,29] 11 days\n",
    "def getSentRes(dfs_daily):\n",
    "    dfs_topic_sent = {}\n",
    "    for d in tqdm(dates):\n",
    "        topic_sent = dfs_daily[d].groupby('maxTopic').apply(lambda x: pd.Series(list(x['sent'])).value_counts())\n",
    "        if isinstance(topic_sent, pd.Series): \n",
    "            df_topic_sent = pd.concat([pd.DataFrame(topic_sent[(i,)]).T for i in range(20) if (i,) in topic_sent.keys()],sort=False).reset_index(drop=True).fillna(0)\n",
    "        else:\n",
    "            df_topic_sent = topic_sent.reset_index(drop=True).fillna(0)#23由于不明情况是df不是series\n",
    "        #  df_topic_sent[[0,1,2]].max(axis=1) # 找出最大Sentiment\n",
    "        df_topic_sent.loc[:,'TopSent'] = [np.argmax([df_topic_sent[0][i],df_topic_sent[1][i],df_topic_sent[2][i]]) for i in range(len(df_topic_sent))]\n",
    "        dfs_topic_sent[d] = df_topic_sent\n",
    "        dfs_topic_sent[d].loc[:,'day']=d\n",
    "    return dfs_topic_sent\n",
    "\n",
    "dfs_topic_sent_tweets =  getSentRes(daily_tweets)\n",
    "dfs_topic_sent_news = getSentRes(daily_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggSentRes(dfs_topic_sent):\n",
    "    df = pd.concat([dfs_topic_sent[d] for d in dates]).reset_index()\n",
    "    df = df.rename(columns={'index':'TopicID'})\n",
    "    df.TopicID = df.TopicID + 1\n",
    "    df.loc[:,'total'] = df[0]+df[1]+df[2]\n",
    "    df.loc[:,'TopicScore'] = (df[2]-df[0])/df['total']\n",
    "    return df.drop(['TopSent'],axis=1)\n",
    "df_topic_sent_tweets = aggSentRes(dfs_topic_sent_tweets)\n",
    "df_topic_sent_news = aggSentRes(dfs_topic_sent_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "startTime\n",
       "2020-0319 00:00:00        0.0\n",
       "2020-0319 06:00:00        0.0\n",
       "2020-0319 12:00:00        0.0\n",
       "2020-0319 18:00:00    42073.0\n",
       "2020-0320 00:00:00    10220.0\n",
       "2020-0320 06:00:00    46156.0\n",
       "2020-0320 12:00:00    57247.0\n",
       "2020-0320 18:00:00    55724.0\n",
       "2020-0321 00:00:00    42418.0\n",
       "2020-0321 06:00:00    42376.0\n",
       "2020-0321 12:00:00    52779.0\n",
       "2020-0321 18:00:00    47679.0\n",
       "2020-0322 00:00:00    41489.0\n",
       "2020-0322 06:00:00    51088.0\n",
       "2020-0322 12:00:00    58297.0\n",
       "2020-0322 18:00:00    49729.0\n",
       "2020-0323 00:00:00    43444.0\n",
       "2020-0323 06:00:00    47476.0\n",
       "2020-0323 12:00:00    64803.0\n",
       "2020-0323 18:00:00    52740.0\n",
       "2020-0324 00:00:00    41977.0\n",
       "2020-0324 06:00:00    48485.0\n",
       "2020-0324 12:00:00    66710.0\n",
       "2020-0324 18:00:00    47739.0\n",
       "2020-0325 00:00:00    35182.0\n",
       "2020-0325 06:00:00    47140.0\n",
       "2020-0325 12:00:00    57767.0\n",
       "2020-0325 18:00:00    44277.0\n",
       "2020-0326 00:00:00    33539.0\n",
       "2020-0326 06:00:00     6923.0\n",
       "2020-0326 12:00:00        0.0\n",
       "2020-0326 18:00:00        0.0\n",
       "2020-0327 00:00:00        0.0\n",
       "2020-0327 06:00:00    44715.0\n",
       "2020-0327 12:00:00    56956.0\n",
       "2020-0327 18:00:00    44087.0\n",
       "2020-0328 00:00:00    34341.0\n",
       "2020-0328 06:00:00    38764.0\n",
       "2020-0328 12:00:00    49964.0\n",
       "2020-0328 18:00:00    39960.0\n",
       "2020-0329 00:00:00    34818.0\n",
       "2020-0329 06:00:00    27425.0\n",
       "2020-0329 12:00:00        0.0\n",
       "2020-0329 18:00:00        0.0\n",
       "Name: total, dtype: float64"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_topic_sent_tweets.groupby('startTime')['total'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./filtered_daily/df_topic_sent_tweets_filtered_subset_1.pickle', 'wb') as handle:\n",
    "    pickle.dump(df_topic_sent_tweets, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('./filtered_daily/df_topic_sent_news_filtered_subset_1.pickle', 'wb') as handle:\n",
    "    pickle.dump(df_topic_sent_news, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TopicID</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>day</th>\n",
       "      <th>total</th>\n",
       "      <th>TopicScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>766</td>\n",
       "      <td>516</td>\n",
       "      <td>89</td>\n",
       "      <td>19</td>\n",
       "      <td>1371</td>\n",
       "      <td>-0.493800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>143</td>\n",
       "      <td>275</td>\n",
       "      <td>27</td>\n",
       "      <td>19</td>\n",
       "      <td>445</td>\n",
       "      <td>-0.260674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>33234</td>\n",
       "      <td>10475</td>\n",
       "      <td>1439</td>\n",
       "      <td>19</td>\n",
       "      <td>45148</td>\n",
       "      <td>-0.704239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>968</td>\n",
       "      <td>409</td>\n",
       "      <td>24</td>\n",
       "      <td>19</td>\n",
       "      <td>1401</td>\n",
       "      <td>-0.673804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>4450</td>\n",
       "      <td>371</td>\n",
       "      <td>377</td>\n",
       "      <td>19</td>\n",
       "      <td>5198</td>\n",
       "      <td>-0.783571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>16</td>\n",
       "      <td>417</td>\n",
       "      <td>187</td>\n",
       "      <td>8</td>\n",
       "      <td>29</td>\n",
       "      <td>612</td>\n",
       "      <td>-0.668301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>17</td>\n",
       "      <td>7419</td>\n",
       "      <td>2150</td>\n",
       "      <td>104</td>\n",
       "      <td>29</td>\n",
       "      <td>9673</td>\n",
       "      <td>-0.756229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>18</td>\n",
       "      <td>62</td>\n",
       "      <td>58</td>\n",
       "      <td>15</td>\n",
       "      <td>29</td>\n",
       "      <td>135</td>\n",
       "      <td>-0.348148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>19</td>\n",
       "      <td>3417</td>\n",
       "      <td>288</td>\n",
       "      <td>52</td>\n",
       "      <td>29</td>\n",
       "      <td>3757</td>\n",
       "      <td>-0.895661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>20</td>\n",
       "      <td>1208</td>\n",
       "      <td>307</td>\n",
       "      <td>285</td>\n",
       "      <td>29</td>\n",
       "      <td>1800</td>\n",
       "      <td>-0.512778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>220 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     TopicID      0      1     2  day  total  TopicScore\n",
       "0          1    766    516    89   19   1371   -0.493800\n",
       "1          2    143    275    27   19    445   -0.260674\n",
       "2          3  33234  10475  1439   19  45148   -0.704239\n",
       "3          4    968    409    24   19   1401   -0.673804\n",
       "4          5   4450    371   377   19   5198   -0.783571\n",
       "..       ...    ...    ...   ...  ...    ...         ...\n",
       "215       16    417    187     8   29    612   -0.668301\n",
       "216       17   7419   2150   104   29   9673   -0.756229\n",
       "217       18     62     58    15   29    135   -0.348148\n",
       "218       19   3417    288    52   29   3757   -0.895661\n",
       "219       20   1208    307   285   29   1800   -0.512778\n",
       "\n",
       "[220 rows x 7 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [np.argmax([df_topic_sent[0][i],df_topic_sent[1][i],df_topic_sent[2][i]]) for i in range(len(df_topic_sent))]\n",
    "df_topic_sent_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TopicID</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>day</th>\n",
       "      <th>total</th>\n",
       "      <th>TopicScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>733</td>\n",
       "      <td>308</td>\n",
       "      <td>153</td>\n",
       "      <td>19</td>\n",
       "      <td>1194</td>\n",
       "      <td>-0.485762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>116</td>\n",
       "      <td>63</td>\n",
       "      <td>98</td>\n",
       "      <td>19</td>\n",
       "      <td>277</td>\n",
       "      <td>-0.064982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>902</td>\n",
       "      <td>184</td>\n",
       "      <td>80</td>\n",
       "      <td>19</td>\n",
       "      <td>1166</td>\n",
       "      <td>-0.704974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1325</td>\n",
       "      <td>294</td>\n",
       "      <td>37</td>\n",
       "      <td>19</td>\n",
       "      <td>1656</td>\n",
       "      <td>-0.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15720</td>\n",
       "      <td>1259</td>\n",
       "      <td>1217</td>\n",
       "      <td>19</td>\n",
       "      <td>18196</td>\n",
       "      <td>-0.797043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>16</td>\n",
       "      <td>518</td>\n",
       "      <td>209</td>\n",
       "      <td>39</td>\n",
       "      <td>29</td>\n",
       "      <td>766</td>\n",
       "      <td>-0.625326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>17</td>\n",
       "      <td>1083</td>\n",
       "      <td>230</td>\n",
       "      <td>24</td>\n",
       "      <td>29</td>\n",
       "      <td>1337</td>\n",
       "      <td>-0.792072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>18</td>\n",
       "      <td>68</td>\n",
       "      <td>58</td>\n",
       "      <td>12</td>\n",
       "      <td>29</td>\n",
       "      <td>138</td>\n",
       "      <td>-0.405797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>19</td>\n",
       "      <td>4042</td>\n",
       "      <td>284</td>\n",
       "      <td>81</td>\n",
       "      <td>29</td>\n",
       "      <td>4407</td>\n",
       "      <td>-0.898797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>20</td>\n",
       "      <td>1737</td>\n",
       "      <td>497</td>\n",
       "      <td>584</td>\n",
       "      <td>29</td>\n",
       "      <td>2818</td>\n",
       "      <td>-0.409155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>220 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     TopicID      0     1     2  day  total  TopicScore\n",
       "0          1    733   308   153   19   1194   -0.485762\n",
       "1          2    116    63    98   19    277   -0.064982\n",
       "2          3    902   184    80   19   1166   -0.704974\n",
       "3          4   1325   294    37   19   1656   -0.777778\n",
       "4          5  15720  1259  1217   19  18196   -0.797043\n",
       "..       ...    ...   ...   ...  ...    ...         ...\n",
       "215       16    518   209    39   29    766   -0.625326\n",
       "216       17   1083   230    24   29   1337   -0.792072\n",
       "217       18     68    58    12   29    138   -0.405797\n",
       "218       19   4042   284    81   29   4407   -0.898797\n",
       "219       20   1737   497   584   29   2818   -0.409155\n",
       "\n",
       "[220 rows x 7 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_topic_sent_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
